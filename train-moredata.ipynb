{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import LinkNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch = 8\n",
    "lr = 1e-4\n",
    "model_path = './data/models/resnet34_003.pth'\n",
    "gamma = 0.35\n",
    "brightness = 2.0\n",
    "colors = 0.25\n",
    "bce_coef = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "def img_size(image: np.ndarray):\n",
    "    return (image.shape[1], image.shape[0])\n",
    "\n",
    "def gauss_noise(img, sigma_squared):\n",
    "    w, h = img_size(img)\n",
    "    gauss = np.random.normal(-sigma_squared, sigma_squared, (h, w, 3))\n",
    "    gauss = gauss.reshape(h, w, 3)\n",
    "    print(gauss.max(), img.max())\n",
    "    img = img + gauss\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "class GaussNoise(object):\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return gauss_noise(img, self.sigma)\n",
    "\n",
    "class AugmentColor(object):\n",
    "    def __init__(self, gamma, brightness, colors):\n",
    "        self.gamma = gamma\n",
    "        self.brightness = brightness\n",
    "        self.colors = colors\n",
    "\n",
    "    def __call__(self, img):\n",
    "        p = np.random.uniform(0, 1, 1)\n",
    "        if p > 0.5:\n",
    "            # randomly shift gamma\n",
    "            random_gamma = torch.from_numpy(np.random.uniform(1-self.gamma, 1+self.gamma, 1)).type(torch.cuda.FloatTensor)\n",
    "            img  = img  ** random_gamma\n",
    "\n",
    "        p = np.random.uniform(0, 1, 1)\n",
    "        if p > 0.5:\n",
    "            # randomly shift brightness\n",
    "            random_brightness =  torch.from_numpy(np.random.uniform(1/self.brightness, self.brightness, 1))\\\n",
    "                .type(torch.cuda.FloatTensor)\n",
    "            img  =  img * random_brightness\n",
    "\n",
    "        p = np.random.uniform(0, 1, 1)\n",
    "        if p > 0.5:\n",
    "            # randomly shift color\n",
    "            random_colors =  torch.from_numpy(np.random.uniform(1-self.colors, 1+self.colors, 3))\\\n",
    "                .type(torch.cuda.FloatTensor)\n",
    "            white = torch.ones([np.shape(img)[1], np.shape(img)[2]]).type(torch.cuda.FloatTensor)\n",
    "            color_image = torch.stack([white * random_colors[i] for i in range(3)], dim=0)\n",
    "            img  *= color_image\n",
    "\n",
    "        # saturate\n",
    "        img  = torch.clamp(img,  0, 1)\n",
    "        return img\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.transform(sample).type(torch.cuda.FloatTensor)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    #GaussNoise(10),\n",
    "    ToTensor(),\n",
    "    AugmentColor(gamma, brightness, colors)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2515 2515\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LyftDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_dir2, img_transform=None, trg_transform=None):\n",
    "        img_dir = os.path.join(data_dir, \"CameraRGB\")\n",
    "        trg_dir = os.path.join(data_dir, \"CameraSeg\")\n",
    "        img_paths = sorted(os.listdir(img_dir))\n",
    "        trg_paths = sorted(os.listdir(trg_dir))\n",
    "        img_paths = [os.path.join(img_dir, path) for path in img_paths]\n",
    "        trg_paths = [os.path.join(trg_dir, path) for path in trg_paths]\n",
    "        \n",
    "        \n",
    "        img_dir2 = os.path.join(data_dir2, \"CameraRGB\")\n",
    "        trg_dir2 = os.path.join(data_dir2, \"CameraSeg\")\n",
    "        img_paths2 = sorted(os.listdir(img_dir2))[::3]\n",
    "        trg_paths2 = sorted(os.listdir(trg_dir2))[::3]\n",
    "        img_paths2 = [os.path.join(img_dir2, path) for path in img_paths2]\n",
    "        trg_paths2 = [os.path.join(trg_dir2, path) for path in trg_paths2]\n",
    "        img_paths.extend(img_paths2)\n",
    "        trg_paths.extend(trg_paths2)\n",
    "        print(len(img_paths), len(trg_paths))\n",
    "        \n",
    "        self.imgs = [cv2.imread(path) for path in img_paths]\n",
    "        self.imgs = [cv2.copyMakeBorder(img,4,4,0,0,cv2.BORDER_REFLECT) for img in self.imgs]\n",
    "        self.trgs = [self._fix_trg(cv2.imread(path)) for path in trg_paths]\n",
    "        self.trgs = [cv2.copyMakeBorder(trg,4,4,0,0,cv2.BORDER_REFLECT) for trg in self.trgs]\n",
    "        self.img_transform = img_transform\n",
    "        self.trg_transform = trg_transform\n",
    "    \n",
    "    def _fix_trg(self, trg):\n",
    "        h, w, _ = trg.shape\n",
    "        mask = np.zeros((h+2, w+2, 1), dtype=np.uint8)\n",
    "        cv2.floodFill(trg, mask, (w//2, h-1), (0,0,0))\n",
    "        vehicles = (trg[:, :, 2]==10).astype(np.float)\n",
    "        road = (trg[:, :, 2]==6).astype(np.float)\n",
    "        road += (trg[:, :, 2]==7).astype(np.float)\n",
    "        bg = np.ones(vehicles.shape) - vehicles - road\n",
    "        return np.stack([bg, road, vehicles], axis=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        trg = self.trgs[idx]\n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "        if self.trg_transform is not None:\n",
    "            trg = self.trg_transform(trg)\n",
    "        return img, trg\n",
    "\n",
    "train_dataset = LyftDataset('./data/', './data/dataset/', train_transform, transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img, trg = train_dataset.__getitem__(80)\n",
    "def show_img(img):\n",
    "    plt.figure(dpi=300)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for data in train_dataset:\n",
    "        print(data[0].shape, data[1].shape)\n",
    "        show_img(np.moveaxis(data[0].cpu().numpy(), 0, -1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import BCEDiceLoss\n",
    "import torch.optim as optim\n",
    "loss_function = BCEDiceLoss(bce_coef).to(device)\n",
    "model = LinkNet34(3, 3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_path = './data/models/resnet34_002_cpt.pth'\n",
    "state = torch.load(load_model_path)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 43.44235575199127 time: 239.30349159240723 s\n",
      "Checkpoint saved\n",
      "Epoch: 2 loss: 26.578725691884756 time: 239.43996047973633 s\n",
      "Checkpoint saved\n",
      "Epoch: 3 loss: 23.246523153036833 time: 239.24568557739258 s\n",
      "Checkpoint saved\n",
      "Epoch: 4 loss: 19.86813148483634 time: 238.11513781547546 s\n",
      "Checkpoint saved\n",
      "Epoch: 5 loss: 18.983006946742535 time: 238.20205163955688 s\n",
      "Checkpoint saved\n",
      "Epoch: 6 loss: 17.822439590469003 time: 238.23431539535522 s\n",
      "Checkpoint saved\n",
      "Epoch: 7 loss: 16.21480198018253 time: 238.25354933738708 s\n",
      "Checkpoint saved\n",
      "Epoch: 8 loss: 15.275330897420645 time: 238.1876220703125 s\n",
      "Checkpoint saved\n",
      "Epoch: 9 loss: 15.054051768034697 time: 238.03970050811768 s\n",
      "Checkpoint saved\n",
      "Epoch: 10 loss: 14.218648981302977 time: 237.84919786453247 s\n",
      "Checkpoint saved\n",
      "Epoch: 11 loss: 13.643306516110897 time: 237.83871960639954 s\n",
      "Checkpoint saved\n",
      "Epoch: 12 loss: 13.2335836738348 time: 237.78394293785095 s\n",
      "Checkpoint saved\n",
      "Epoch: 13 loss: 12.904990680515766 time: 237.91810822486877 s\n",
      "Checkpoint saved\n",
      "Epoch: 14 loss: 12.788451502099633 time: 238.1872673034668 s\n",
      "Checkpoint saved\n",
      "Epoch: 15 loss: 13.906510373577476 time: 238.34820795059204 s\n",
      "Epoch: 16 loss: 13.411184709519148 time: 238.3228361606598 s\n",
      "Epoch: 17 loss: 12.194152789190412 time: 238.12440299987793 s\n",
      "Checkpoint saved\n",
      "Epoch: 18 loss: 11.928161235526204 time: 238.1502664089203 s\n",
      "Checkpoint saved\n",
      "Epoch: 19 loss: 11.53395333327353 time: 238.11433863639832 s\n",
      "Checkpoint saved\n",
      "Epoch: 20 loss: 11.228587793186307 time: 237.79554510116577 s\n",
      "Checkpoint saved\n",
      "Epoch: 21 loss: 11.050389908254147 time: 237.92889952659607 s\n",
      "Checkpoint saved\n",
      "Epoch: 22 loss: 10.796904236078262 time: 237.87183666229248 s\n",
      "Checkpoint saved\n",
      "Epoch: 23 loss: 10.676769684068859 time: 237.7872495651245 s\n",
      "Checkpoint saved\n",
      "Epoch: 24 loss: 11.225154586136341 time: 237.89621591567993 s\n",
      "Epoch: 25 loss: 10.61463712155819 time: 237.88351321220398 s\n",
      "Checkpoint saved\n",
      "Epoch: 26 loss: 10.422103626653552 time: 237.8673300743103 s\n",
      "Checkpoint saved\n",
      "Epoch: 27 loss: 10.12984485924244 time: 237.92284607887268 s\n",
      "Checkpoint saved\n",
      "Epoch: 28 loss: 10.01987105421722 time: 237.89269495010376 s\n",
      "Checkpoint saved\n",
      "Epoch: 29 loss: 9.919281216338277 time: 237.88025569915771 s\n",
      "Checkpoint saved\n",
      "Epoch: 30 loss: 9.926491309888661 time: 237.83087539672852 s\n",
      "Epoch: 31 loss: 10.303645445033908 time: 237.8423991203308 s\n",
      "Epoch: 32 loss: 10.009856978431344 time: 237.7846221923828 s\n",
      "Epoch: 33 loss: 9.580813409760594 time: 237.77414870262146 s\n",
      "Checkpoint saved\n",
      "Epoch: 34 loss: 10.250194570049644 time: 237.82276225090027 s\n",
      "Epoch: 35 loss: 9.566440895199776 time: 237.67696332931519 s\n",
      "Checkpoint saved\n",
      "Epoch: 36 loss: 9.185418095439672 time: 237.68416714668274 s\n",
      "Checkpoint saved\n",
      "Epoch: 37 loss: 8.93639279063791 time: 237.76899409294128 s\n",
      "Checkpoint saved\n",
      "Epoch: 38 loss: 8.972767231985927 time: 237.82283878326416 s\n",
      "Epoch: 39 loss: 9.075321046635509 time: 237.859037399292 s\n",
      "Epoch: 40 loss: 9.050327070057392 time: 237.74873089790344 s\n",
      "Epoch: 41 loss: 8.865316407755017 time: 237.85347938537598 s\n",
      "Checkpoint saved\n",
      "Epoch: 42 loss: 8.812867322936654 time: 237.82013273239136 s\n",
      "Checkpoint saved\n",
      "Epoch: 43 loss: 8.71034175530076 time: 237.80524063110352 s\n",
      "Checkpoint saved\n",
      "Epoch: 44 loss: 8.727871754206717 time: 237.8146059513092 s\n",
      "Epoch: 45 loss: 8.648622347041965 time: 237.81253218650818 s\n",
      "Checkpoint saved\n",
      "Epoch: 46 loss: 8.831802627071738 time: 237.7469551563263 s\n",
      "Epoch: 47 loss: 8.45689260493964 time: 238.0139675140381 s\n",
      "Checkpoint saved\n",
      "Epoch: 48 loss: 8.347211811691523 time: 237.8109850883484 s\n",
      "Checkpoint saved\n",
      "Epoch: 49 loss: 8.264489501714706 time: 237.80231952667236 s\n",
      "Checkpoint saved\n",
      "Epoch: 50 loss: 8.256863093934953 time: 237.80578899383545 s\n",
      "Checkpoint saved\n",
      "Epoch: 51 loss: 8.241939120925963 time: 237.72437357902527 s\n",
      "Checkpoint saved\n",
      "Epoch: 52 loss: 9.216427901759744 time: 237.69063878059387 s\n",
      "Epoch: 53 loss: 8.883908730000257 time: 237.69815802574158 s\n",
      "Epoch: 54 loss: 7.984007993713021 time: 237.87307834625244 s\n",
      "Checkpoint saved\n",
      "Epoch: 55 loss: 7.815881684422493 time: 237.73958897590637 s\n",
      "Checkpoint saved\n",
      "Epoch: 56 loss: 7.7285719234496355 time: 237.95940113067627 s\n",
      "Checkpoint saved\n",
      "Epoch: 57 loss: 7.691472572274506 time: 237.79918456077576 s\n",
      "Checkpoint saved\n",
      "Epoch: 58 loss: 7.778903527185321 time: 237.751362323761 s\n",
      "Epoch: 59 loss: 7.835460786707699 time: 237.76647543907166 s\n",
      "Epoch: 60 loss: 7.821797102689743 time: 237.8541829586029 s\n",
      "Epoch: 61 loss: 7.726347237825394 time: 237.736656665802 s\n",
      "Epoch: 62 loss: 7.724363472312689 time: 237.71928429603577 s\n",
      "Epoch: 63 loss: 7.766858397983015 time: 237.9105260372162 s\n",
      "Epoch: 64 loss: 7.625623513944447 time: 237.7644944190979 s\n",
      "Checkpoint saved\n",
      "Epoch: 65 loss: 7.460424146614969 time: 237.9038770198822 s\n",
      "Checkpoint saved\n",
      "Epoch: 66 loss: 7.587259360589087 time: 237.8398027420044 s\n",
      "Epoch: 67 loss: 7.4531710129231215 time: 237.82927060127258 s\n",
      "Checkpoint saved\n",
      "Epoch: 68 loss: 7.4629790261387825 time: 237.93950963020325 s\n",
      "Epoch: 69 loss: 7.476153532974422 time: 237.71418714523315 s\n",
      "Epoch: 70 loss: 7.419022376649082 time: 237.70188570022583 s\n",
      "Checkpoint saved\n",
      "Epoch: 71 loss: 7.243162431754172 time: 237.78503131866455 s\n",
      "Checkpoint saved\n",
      "Epoch: 72 loss: 7.429777830839157 time: 237.747731924057 s\n",
      "Epoch: 73 loss: 7.41551074013114 time: 237.82829213142395 s\n",
      "Epoch: 74 loss: 7.223402678966522 time: 237.7736415863037 s\n",
      "Checkpoint saved\n",
      "Epoch: 75 loss: 7.198628956452012 time: 237.85006880760193 s\n",
      "Checkpoint saved\n",
      "Epoch: 76 loss: 7.142257200554013 time: 237.8914656639099 s\n",
      "Checkpoint saved\n",
      "Epoch: 77 loss: 7.129398279823363 time: 237.87789106369019 s\n",
      "Checkpoint saved\n",
      "Epoch: 78 loss: 7.049991776235402 time: 237.8683843612671 s\n",
      "Checkpoint saved\n",
      "Epoch: 79 loss: 7.106896111741662 time: 238.08307886123657 s\n",
      "Epoch: 80 loss: 7.028609343804419 time: 237.9702489376068 s\n",
      "Checkpoint saved\n",
      "Epoch: 81 loss: 6.988732525147498 time: 237.8533730506897 s\n",
      "Checkpoint saved\n",
      "Epoch: 82 loss: 6.884199399501085 time: 237.85933375358582 s\n",
      "Checkpoint saved\n",
      "Epoch: 83 loss: 6.839349782094359 time: 238.16754388809204 s\n",
      "Checkpoint saved\n",
      "Epoch: 84 loss: 6.796832143329084 time: 238.1355037689209 s\n",
      "Checkpoint saved\n",
      "Epoch: 85 loss: 6.8031261414289474 time: 237.99720883369446 s\n",
      "Epoch: 86 loss: 6.828702569939196 time: 237.9395842552185 s\n",
      "Epoch: 87 loss: 6.744442189112306 time: 237.93701696395874 s\n",
      "Checkpoint saved\n",
      "Epoch: 88 loss: 6.746331445872784 time: 237.91616773605347 s\n",
      "Epoch: 89 loss: 6.697208418510854 time: 238.04370069503784 s\n",
      "Checkpoint saved\n",
      "Epoch: 90 loss: 6.66256685834378 time: 238.02055859565735 s\n",
      "Checkpoint saved\n",
      "Epoch: 91 loss: 6.675255413167179 time: 238.17099022865295 s\n",
      "Epoch: 92 loss: 6.580938398838043 time: 238.04968690872192 s\n",
      "Checkpoint saved\n",
      "Epoch: 93 loss: 6.580907016992569 time: 239.53084230422974 s\n",
      "Checkpoint saved\n",
      "Epoch: 94 loss: 6.6503822058439255 time: 239.5778410434723 s\n",
      "Epoch: 95 loss: 6.790206839330494 time: 238.3415732383728 s\n",
      "Epoch: 96 loss: 6.5371803771704435 time: 238.45618534088135 s\n",
      "Checkpoint saved\n",
      "Epoch: 97 loss: 6.398900089785457 time: 238.42674851417542 s\n",
      "Checkpoint saved\n",
      "Epoch: 98 loss: 6.341873464174569 time: 239.33233451843262 s\n",
      "Checkpoint saved\n",
      "Epoch: 99 loss: 6.404909861274064 time: 237.91459012031555 s\n",
      "Epoch: 100 loss: 6.467316967435181 time: 237.76977682113647 s\n",
      "Epoch: 101 loss: 6.336922904476523 time: 237.6908404827118 s\n",
      "Checkpoint saved\n",
      "Epoch: 102 loss: 6.2839479967951775 time: 237.70810413360596 s\n",
      "Checkpoint saved\n",
      "Epoch: 103 loss: 6.579148298129439 time: 237.6486155986786 s\n",
      "Epoch: 104 loss: 6.332511030137539 time: 237.7128210067749 s\n",
      "Epoch: 105 loss: 6.239242254756391 time: 238.05880570411682 s\n",
      "Checkpoint saved\n",
      "Epoch: 106 loss: 6.321410548873246 time: 237.77571320533752 s\n",
      "Epoch: 107 loss: 6.214000143110752 time: 237.77415704727173 s\n",
      "Checkpoint saved\n",
      "Epoch: 108 loss: 6.157045365311205 time: 237.66833090782166 s\n",
      "Checkpoint saved\n",
      "Epoch: 109 loss: 6.577893728390336 time: 237.66698265075684 s\n",
      "Epoch: 110 loss: 6.196667739190161 time: 237.67629313468933 s\n",
      "Epoch: 111 loss: 6.088874721899629 time: 237.65410208702087 s\n",
      "Checkpoint saved\n",
      "Epoch: 112 loss: 6.004361207596958 time: 237.63998174667358 s\n",
      "Checkpoint saved\n",
      "Epoch: 113 loss: 6.021784477867186 time: 237.71691846847534 s\n",
      "Epoch: 114 loss: 9.933063636533916 time: 237.6967053413391 s\n",
      "Epoch: 115 loss: 14.416956325992942 time: 237.627459526062 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 loss: 9.021850612014532 time: 237.6672487258911 s\n",
      "Epoch: 117 loss: 7.390742231160402 time: 237.68674659729004 s\n",
      "Epoch: 118 loss: 6.671435027383268 time: 237.691077709198 s\n",
      "Epoch: 119 loss: 6.249731785617769 time: 237.59751915931702 s\n",
      "Epoch: 120 loss: 5.965912719257176 time: 237.75020933151245 s\n",
      "Checkpoint saved\n",
      "Epoch: 121 loss: 5.825400850735605 time: 237.67586970329285 s\n",
      "Checkpoint saved\n",
      "Epoch: 122 loss: 5.787101532332599 time: 237.65906882286072 s\n",
      "Checkpoint saved\n",
      "Epoch: 123 loss: 5.734213276766241 time: 237.69465398788452 s\n",
      "Checkpoint saved\n",
      "Epoch: 124 loss: 5.663996180519462 time: 237.74854111671448 s\n",
      "Checkpoint saved\n",
      "Epoch: 125 loss: 5.675771647132933 time: 237.64060044288635 s\n",
      "Epoch: 126 loss: 5.6344291446730494 time: 237.68607759475708 s\n",
      "Checkpoint saved\n",
      "Epoch: 127 loss: 5.706817157566547 time: 237.6790907382965 s\n",
      "Epoch: 128 loss: 5.795093893073499 time: 237.75051069259644 s\n",
      "Epoch: 129 loss: 5.801473788917065 time: 237.7179775238037 s\n",
      "Epoch: 130 loss: 5.916758148930967 time: 237.72284960746765 s\n",
      "Epoch: 131 loss: 5.79971569404006 time: 237.65846490859985 s\n",
      "Epoch: 132 loss: 6.302787019871175 time: 237.7094690799713 s\n",
      "Epoch: 133 loss: 5.883102803491056 time: 237.72733068466187 s\n",
      "Epoch: 134 loss: 5.717168688774109 time: 237.74091291427612 s\n",
      "Epoch: 135 loss: 5.864156569354236 time: 237.75949048995972 s\n",
      "Epoch: 136 loss: 5.8204139759764075 time: 237.647141456604 s\n",
      "Epoch: 137 loss: 5.905370294116437 time: 237.6835503578186 s\n",
      "Epoch: 138 loss: 5.71896611712873 time: 237.5964412689209 s\n",
      "Epoch: 139 loss: 5.69326036144048 time: 237.63797211647034 s\n",
      "Epoch: 140 loss: 13.500255171209574 time: 237.57657074928284 s\n",
      "Epoch: 141 loss: 12.447086714208126 time: 237.53309035301208 s\n",
      "Epoch: 142 loss: 8.501145675778389 time: 237.62307715415955 s\n",
      "Epoch: 143 loss: 7.424677713774145 time: 237.6770622730255 s\n",
      "Epoch: 144 loss: 6.554753699339926 time: 237.6120479106903 s\n",
      "Epoch: 145 loss: 6.156108145602047 time: 237.60772681236267 s\n",
      "Epoch: 146 loss: 6.097380544990301 time: 237.6359362602234 s\n",
      "Epoch: 147 loss: 5.795159287750721 time: 237.69128394126892 s\n",
      "Epoch: 148 loss: 5.6054606242105365 time: 237.6783127784729 s\n",
      "Checkpoint saved\n",
      "Epoch: 149 loss: 5.529441928490996 time: 237.6915786266327 s\n",
      "Checkpoint saved\n",
      "Epoch: 150 loss: 5.58338469825685 time: 237.6772198677063 s\n",
      "Epoch: 151 loss: 5.463867777958512 time: 237.70388793945312 s\n",
      "Checkpoint saved\n",
      "Epoch: 152 loss: 5.360838007181883 time: 237.72581100463867 s\n",
      "Checkpoint saved\n",
      "Epoch: 153 loss: 5.317832936532795 time: 237.62681794166565 s\n",
      "Checkpoint saved\n",
      "Epoch: 154 loss: 5.393369322642684 time: 237.73281455039978 s\n",
      "Epoch: 155 loss: 5.410265335813165 time: 237.76745676994324 s\n",
      "Epoch: 156 loss: 5.474158146418631 time: 237.75635838508606 s\n",
      "Epoch: 157 loss: 5.423547059297562 time: 237.7039291858673 s\n",
      "Epoch: 158 loss: 5.504152677953243 time: 237.67151165008545 s\n",
      "Epoch: 159 loss: 5.596398565918207 time: 237.78211784362793 s\n",
      "Epoch: 160 loss: 5.611076805740595 time: 237.74442434310913 s\n",
      "Epoch: 161 loss: 5.542944949120283 time: 237.67275071144104 s\n",
      "Epoch: 162 loss: 5.5333831226453185 time: 237.68587040901184 s\n",
      "Epoch: 163 loss: 5.640198443084955 time: 237.73693823814392 s\n",
      "Epoch: 164 loss: 5.723412949591875 time: 237.77199387550354 s\n",
      "Epoch: 165 loss: 5.636745364405215 time: 237.80268120765686 s\n",
      "Epoch: 166 loss: 5.4880452221259475 time: 237.70767068862915 s\n",
      "Epoch: 167 loss: 5.552523782476783 time: 237.71529006958008 s\n",
      "Epoch: 168 loss: 5.501467132940888 time: 237.74978137016296 s\n",
      "Epoch: 169 loss: 5.524153656326234 time: 237.86263513565063 s\n",
      "Epoch: 170 loss: 5.426237167790532 time: 237.81123518943787 s\n",
      "Epoch: 171 loss: 5.45877649076283 time: 237.81713485717773 s\n",
      "Epoch: 172 loss: 5.393241840414703 time: 237.82561683654785 s\n",
      "Epoch: 173 loss: 5.427766557782888 time: 237.68798279762268 s\n",
      "Epoch: 174 loss: 5.417201014235616 time: 237.76577234268188 s\n",
      "Epoch: 175 loss: 5.443211079575121 time: 237.7392590045929 s\n",
      "Epoch: 176 loss: 5.379980232566595 time: 237.79682207107544 s\n",
      "Epoch: 177 loss: 5.331669512204826 time: 237.7300682067871 s\n",
      "Epoch: 178 loss: 5.480966788716614 time: 237.7506971359253 s\n",
      "Epoch: 179 loss: 5.368954408913851 time: 237.672532081604 s\n",
      "Epoch: 180 loss: 5.312410110607743 time: 237.79820799827576 s\n",
      "Checkpoint saved\n",
      "Epoch: 181 loss: 5.232785559259355 time: 237.78255152702332 s\n",
      "Checkpoint saved\n",
      "Epoch: 182 loss: 5.246355445124209 time: 237.80280232429504 s\n",
      "Epoch: 183 loss: 5.335435562767088 time: 237.763263463974 s\n",
      "Epoch: 184 loss: 5.207907820120454 time: 237.8322296142578 s\n",
      "Checkpoint saved\n",
      "Epoch: 185 loss: 5.197244670242071 time: 237.89923810958862 s\n",
      "Checkpoint saved\n",
      "Epoch: 186 loss: 5.208580375649035 time: 237.80885410308838 s\n",
      "Epoch: 187 loss: 5.1936766700819135 time: 237.82323575019836 s\n",
      "Checkpoint saved\n",
      "Epoch: 188 loss: 5.191138248890638 time: 237.79236507415771 s\n",
      "Checkpoint saved\n",
      "Epoch: 189 loss: 5.200387045741081 time: 237.87600803375244 s\n",
      "Epoch: 190 loss: 5.178549587726593 time: 237.84691071510315 s\n",
      "Checkpoint saved\n",
      "Epoch: 191 loss: 5.136019458062947 time: 237.81236577033997 s\n",
      "Checkpoint saved\n",
      "Epoch: 192 loss: 5.127057857811451 time: 237.83957052230835 s\n",
      "Checkpoint saved\n",
      "Epoch: 193 loss: 5.149624979123473 time: 237.78036904335022 s\n",
      "Epoch: 194 loss: 5.05967041105032 time: 237.83121919631958 s\n",
      "Checkpoint saved\n",
      "Epoch: 195 loss: 5.0785236563533545 time: 237.85346341133118 s\n",
      "Epoch: 196 loss: 5.077227904461324 time: 237.75540375709534 s\n",
      "Epoch: 197 loss: 5.100759938359261 time: 237.82091760635376 s\n",
      "Epoch: 198 loss: 5.095373994670808 time: 237.7257878780365 s\n",
      "Epoch: 199 loss: 5.188128314912319 time: 237.84798455238342 s\n",
      "Epoch: 200 loss: 5.068397262133658 time: 237.8930048942566 s\n",
      "Epoch: 201 loss: 5.0418323846533895 time: 237.84615302085876 s\n",
      "Checkpoint saved\n",
      "Epoch: 202 loss: 4.9644846664741635 time: 237.78833198547363 s\n",
      "Checkpoint saved\n",
      "Epoch: 203 loss: 4.974885412491858 time: 237.7324938774109 s\n",
      "Epoch: 204 loss: 4.92938378918916 time: 237.72932481765747 s\n",
      "Checkpoint saved\n",
      "Epoch: 205 loss: 4.899456788785756 time: 237.8099594116211 s\n",
      "Checkpoint saved\n",
      "Epoch: 206 loss: 5.002331337891519 time: 237.88536429405212 s\n",
      "Epoch: 207 loss: 4.9592719580978155 time: 237.8366503715515 s\n",
      "Epoch: 208 loss: 4.888789814896882 time: 237.77369499206543 s\n",
      "Checkpoint saved\n",
      "Epoch: 209 loss: 4.857682866975665 time: 237.79416370391846 s\n",
      "Checkpoint saved\n",
      "Epoch: 210 loss: 5.005078982561827 time: 237.69410943984985 s\n",
      "Epoch: 211 loss: 5.050164264626801 time: 237.89562606811523 s\n",
      "Epoch: 212 loss: 4.888041107915342 time: 237.81485986709595 s\n",
      "Epoch: 213 loss: 4.853685106150806 time: 237.8779594898224 s\n",
      "Checkpoint saved\n",
      "Epoch: 214 loss: 4.793487461283803 time: 237.82881450653076 s\n",
      "Checkpoint saved\n",
      "Epoch: 215 loss: 4.8976012747734785 time: 237.82671189308167 s\n",
      "Epoch: 216 loss: 4.7733732759952545 time: 237.78364634513855 s\n",
      "Checkpoint saved\n",
      "Epoch: 217 loss: 4.737760240677744 time: 237.85394859313965 s\n",
      "Checkpoint saved\n",
      "Epoch: 218 loss: 4.785794283263385 time: 237.89907598495483 s\n",
      "Epoch: 219 loss: 4.861902320757508 time: 237.9484350681305 s\n",
      "Epoch: 220 loss: 4.8094439171254635 time: 237.92296886444092 s\n",
      "Epoch: 221 loss: 4.810253539122641 time: 237.9129774570465 s\n",
      "Epoch: 222 loss: 4.7562357652932405 time: 237.77703309059143 s\n",
      "Epoch: 223 loss: 4.712190763093531 time: 237.9564254283905 s\n",
      "Checkpoint saved\n",
      "Epoch: 224 loss: 4.698953030630946 time: 237.97048354148865 s\n",
      "Checkpoint saved\n",
      "Epoch: 225 loss: 6.296198350377381 time: 237.838711977005 s\n",
      "Epoch: 226 loss: 4.989405850879848 time: 237.84036254882812 s\n",
      "Epoch: 227 loss: 4.568616348318756 time: 237.87744688987732 s\n",
      "Checkpoint saved\n",
      "Epoch: 228 loss: 4.458643603138626 time: 237.87125158309937 s\n",
      "Checkpoint saved\n",
      "Epoch: 229 loss: 4.430087707005441 time: 237.8444561958313 s\n",
      "Checkpoint saved\n",
      "Epoch: 230 loss: 4.4611123451031744 time: 237.84877943992615 s\n",
      "Epoch: 231 loss: 4.514394306577742 time: 237.92798042297363 s\n",
      "Epoch: 232 loss: 4.557390362024307 time: 237.8927013874054 s\n",
      "Epoch: 233 loss: 4.6151003846898675 time: 237.82204031944275 s\n",
      "Epoch: 234 loss: 7.7360037956386805 time: 237.7680230140686 s\n",
      "Epoch: 235 loss: 7.5586570631712675 time: 237.77646803855896 s\n",
      "Epoch: 236 loss: 5.391358148306608 time: 237.79881405830383 s\n",
      "Epoch: 237 loss: 4.783413153141737 time: 237.7276668548584 s\n",
      "Epoch: 238 loss: 4.598501395434141 time: 237.78859162330627 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 loss: 4.41332864202559 time: 237.78272032737732 s\n",
      "Checkpoint saved\n",
      "Epoch: 240 loss: 4.445261163637042 time: 237.7391972541809 s\n",
      "Epoch: 241 loss: 4.319328165613115 time: 237.78627347946167 s\n",
      "Checkpoint saved\n",
      "Epoch: 242 loss: 4.297984664328396 time: 237.83015942573547 s\n",
      "Checkpoint saved\n",
      "Epoch: 243 loss: 4.447765822522342 time: 237.85625529289246 s\n",
      "Epoch: 244 loss: 4.400516036432236 time: 237.82069897651672 s\n",
      "Epoch: 245 loss: 4.459273950196803 time: 237.7625949382782 s\n",
      "Epoch: 246 loss: 4.451233308296651 time: 237.81122994422913 s\n",
      "Epoch: 247 loss: 4.541255367454141 time: 237.68673372268677 s\n",
      "Epoch: 248 loss: 4.580882400274277 time: 237.75545382499695 s\n",
      "Epoch: 249 loss: 4.55484358407557 time: 238.19212746620178 s\n",
      "Epoch: 250 loss: 4.566498324740678 time: 237.8260943889618 s\n",
      "Epoch: 251 loss: 4.630310460925102 time: 237.91403365135193 s\n",
      "Epoch: 252 loss: 4.621210774406791 time: 237.90225172042847 s\n",
      "Epoch: 253 loss: 4.603346002288163 time: 237.78777742385864 s\n",
      "Epoch: 254 loss: 4.546672070398927 time: 237.82144737243652 s\n",
      "Epoch: 255 loss: 4.479232859797776 time: 237.86647605895996 s\n",
      "Epoch: 256 loss: 4.443494176492095 time: 237.77109837532043 s\n",
      "Epoch: 257 loss: 4.502584814094007 time: 237.85272765159607 s\n",
      "Epoch: 258 loss: 4.511824043467641 time: 237.7927324771881 s\n",
      "Epoch: 259 loss: 4.50666787661612 time: 237.85301113128662 s\n",
      "Epoch: 260 loss: 4.481130784377456 time: 237.86988043785095 s\n",
      "Epoch: 261 loss: 4.441737083252519 time: 237.77497243881226 s\n",
      "Epoch: 262 loss: 4.447602664120495 time: 237.82185125350952 s\n",
      "Epoch: 263 loss: 4.473528936505318 time: 237.77303814888 s\n",
      "Epoch: 264 loss: 4.519556704908609 time: 237.78029108047485 s\n",
      "Epoch: 265 loss: 4.418503220193088 time: 237.81081700325012 s\n",
      "Epoch: 266 loss: 4.4210687428712845 time: 237.82936596870422 s\n",
      "Epoch: 267 loss: 4.423022690229118 time: 237.8018455505371 s\n",
      "Epoch: 268 loss: 4.367261667735875 time: 237.762375831604 s\n",
      "Epoch: 269 loss: 4.353063603397459 time: 237.76395964622498 s\n",
      "Epoch: 270 loss: 4.4258070779033005 time: 238.69475030899048 s\n",
      "Epoch: 271 loss: 4.372231228277087 time: 241.652259349823 s\n",
      "Epoch: 272 loss: 4.382296248339117 time: 238.3406023979187 s\n",
      "Epoch: 273 loss: 4.372199513949454 time: 238.3779170513153 s\n",
      "Epoch: 274 loss: 4.331389430910349 time: 238.3664093017578 s\n",
      "Epoch: 275 loss: 4.7749184696003795 time: 238.23844575881958 s\n",
      "Epoch: 276 loss: 7.814909032545984 time: 238.22223448753357 s\n",
      "Epoch: 277 loss: 5.204270397312939 time: 238.16056323051453 s\n",
      "Epoch: 278 loss: 4.429662648588419 time: 238.16888451576233 s\n",
      "Epoch: 279 loss: 4.223525008186698 time: 238.1396849155426 s\n",
      "Checkpoint saved\n",
      "Epoch: 280 loss: 4.097603005822748 time: 238.3695158958435 s\n",
      "Checkpoint saved\n",
      "Epoch: 281 loss: 4.029993735719472 time: 238.20693826675415 s\n",
      "Checkpoint saved\n",
      "Epoch: 282 loss: 4.039309639018029 time: 238.2186005115509 s\n",
      "Epoch: 283 loss: 4.083584425970912 time: 238.24220514297485 s\n",
      "Epoch: 284 loss: 4.120041054673493 time: 238.14973711967468 s\n",
      "Epoch: 285 loss: 4.14740731427446 time: 238.16109013557434 s\n",
      "Epoch: 286 loss: 4.339109733700752 time: 238.24447178840637 s\n",
      "Epoch: 287 loss: 4.218724534381181 time: 238.23743772506714 s\n",
      "Epoch: 288 loss: 4.230850362684578 time: 238.28641271591187 s\n",
      "Epoch: 289 loss: 4.301375960931182 time: 238.198734998703 s\n",
      "Epoch: 290 loss: 4.308142859488726 time: 238.33683109283447 s\n",
      "Epoch: 291 loss: 4.322224681265652 time: 238.33536767959595 s\n",
      "Epoch: 292 loss: 4.2274228017777205 time: 238.24216532707214 s\n",
      "Epoch: 293 loss: 4.2123179994523525 time: 238.22106075286865 s\n",
      "Epoch: 294 loss: 4.201879889704287 time: 238.4154303073883 s\n",
      "Epoch: 295 loss: 4.2475810456089675 time: 238.2612590789795 s\n",
      "Epoch: 296 loss: 4.184194983448833 time: 238.13333582878113 s\n",
      "Epoch: 297 loss: 4.151008595246822 time: 238.1616644859314 s\n",
      "Epoch: 298 loss: 4.193691075779498 time: 238.9294764995575 s\n",
      "Epoch: 299 loss: 4.206002940423787 time: 240.2885251045227 s\n",
      "Epoch: 300 loss: 4.172820877749473 time: 239.32727813720703 s\n",
      "Epoch: 301 loss: 4.225586014334112 time: 239.55272269248962 s\n",
      "Epoch: 302 loss: 4.2151899533346295 time: 239.9551501274109 s\n",
      "Epoch: 303 loss: 4.2471964112482965 time: 240.86017894744873 s\n",
      "Epoch: 304 loss: 4.128755924291909 time: 239.29787921905518 s\n",
      "Epoch: 305 loss: 4.092297750990838 time: 239.16139602661133 s\n",
      "Epoch: 306 loss: 4.101247769780457 time: 240.52313232421875 s\n",
      "Epoch: 307 loss: 4.124775668606162 time: 239.85592103004456 s\n",
      "Epoch: 308 loss: 4.065557694993913 time: 239.63627815246582 s\n",
      "Epoch: 309 loss: 4.123529034666717 time: 239.54512929916382 s\n",
      "Epoch: 310 loss: 4.138474840205163 time: 238.66418957710266 s\n",
      "Epoch: 311 loss: 4.065314275678247 time: 238.54448676109314 s\n",
      "Epoch: 312 loss: 4.081845776177943 time: 238.48378324508667 s\n",
      "Epoch: 313 loss: 4.077884249854833 time: 239.71789169311523 s\n",
      "Epoch: 314 loss: 4.055064530111849 time: 242.37184500694275 s\n",
      "Epoch: 315 loss: 4.0710079236887395 time: 243.75363397598267 s\n",
      "Epoch: 316 loss: 4.752585367765278 time: 242.43828105926514 s\n",
      "Epoch: 317 loss: 4.123210360761732 time: 242.20338320732117 s\n",
      "Epoch: 318 loss: 3.9591430397704244 time: 239.93403267860413 s\n",
      "Checkpoint saved\n",
      "Epoch: 319 loss: 3.9158590119332075 time: 238.39268517494202 s\n",
      "Checkpoint saved\n",
      "Epoch: 320 loss: 4.016450149938464 time: 238.70733428001404 s\n",
      "Epoch: 321 loss: 3.974427575711161 time: 238.17072010040283 s\n",
      "Epoch: 322 loss: 4.090918689966202 time: 238.1151008605957 s\n",
      "Epoch: 323 loss: 3.995254924055189 time: 238.1837706565857 s\n",
      "Epoch: 324 loss: 3.932042765431106 time: 240.63763737678528 s\n",
      "Epoch: 325 loss: 3.9773074840195477 time: 242.96982145309448 s\n",
      "Epoch: 326 loss: 4.3370077782310545 time: 242.9051365852356 s\n",
      "Epoch: 327 loss: 4.239965740125626 time: 242.54012894630432 s\n",
      "Epoch: 328 loss: 4.106882211286575 time: 240.7505760192871 s\n",
      "Epoch: 329 loss: 3.8793691163882613 time: 240.40777373313904 s\n",
      "Checkpoint saved\n",
      "Epoch: 330 loss: 3.8593199173919857 time: 243.39657831192017 s\n",
      "Checkpoint saved\n",
      "Epoch: 331 loss: 3.8590629622340202 time: 241.40938663482666 s\n",
      "Checkpoint saved\n",
      "Epoch: 332 loss: 3.9252982125617564 time: 244.21904754638672 s\n",
      "Epoch: 333 loss: 4.0219859653152525 time: 241.17987489700317 s\n",
      "Epoch: 334 loss: 3.904655340593308 time: 240.32366943359375 s\n",
      "Epoch: 335 loss: 3.968285445123911 time: 238.77370190620422 s\n",
      "Epoch: 336 loss: 3.9003797117620707 time: 238.91946935653687 s\n",
      "Epoch: 337 loss: 4.152231497690082 time: 238.77784514427185 s\n",
      "Epoch: 338 loss: 3.953596828505397 time: 238.28917002677917 s\n",
      "Epoch: 339 loss: 3.856064728461206 time: 238.47731804847717 s\n",
      "Checkpoint saved\n",
      "Epoch: 340 loss: 3.841331632807851 time: 238.48417782783508 s\n",
      "Checkpoint saved\n",
      "Epoch: 341 loss: 3.838098296429962 time: 238.3807189464569 s\n",
      "Checkpoint saved\n",
      "Epoch: 342 loss: 3.8275233483873308 time: 238.30097246170044 s\n",
      "Checkpoint saved\n",
      "Epoch: 343 loss: 3.9176908265799284 time: 238.34473204612732 s\n",
      "Epoch: 344 loss: 3.927123815752566 time: 238.357164144516 s\n",
      "Epoch: 345 loss: 3.9261650913394988 time: 238.34186100959778 s\n",
      "Epoch: 346 loss: 4.082094385754317 time: 238.43769359588623 s\n",
      "Epoch: 347 loss: 4.2389938603155315 time: 238.46922945976257 s\n",
      "Epoch: 348 loss: 3.863807258196175 time: 238.11617469787598 s\n",
      "Epoch: 349 loss: 3.7431182274594903 time: 238.05931091308594 s\n",
      "Checkpoint saved\n",
      "Epoch: 350 loss: 3.7363482895307243 time: 238.05406284332275 s\n",
      "Checkpoint saved\n",
      "Epoch: 351 loss: 3.796165795996785 time: 238.07850694656372 s\n",
      "Epoch: 352 loss: 3.8206403967924416 time: 238.0212848186493 s\n",
      "Epoch: 353 loss: 3.8161935759708285 time: 238.09665036201477 s\n",
      "Epoch: 354 loss: 3.7813343456946313 time: 238.0402534008026 s\n",
      "Epoch: 355 loss: 3.7756877932697535 time: 238.01735162734985 s\n",
      "Epoch: 356 loss: 3.811761349439621 time: 238.06226992607117 s\n",
      "Epoch: 357 loss: 3.7825160548090935 time: 238.01814532279968 s\n",
      "Epoch: 358 loss: 4.0913951667025685 time: 238.01280903816223 s\n",
      "Epoch: 359 loss: 3.884636731352657 time: 238.06237530708313 s\n",
      "Epoch: 360 loss: 3.70932662114501 time: 237.99194359779358 s\n",
      "Checkpoint saved\n",
      "Epoch: 361 loss: 3.756547389551997 time: 237.9375762939453 s\n",
      "Epoch: 362 loss: 3.790770314168185 time: 238.01419305801392 s\n",
      "Epoch: 363 loss: 3.7846331386826932 time: 238.2499623298645 s\n",
      "Epoch: 364 loss: 3.7220829273574054 time: 238.04906129837036 s\n",
      "Epoch: 365 loss: 3.73157126782462 time: 238.0095500946045 s\n",
      "Epoch: 366 loss: 3.735072623472661 time: 238.0646731853485 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 367 loss: 3.76908414112404 time: 238.06935811042786 s\n",
      "Epoch: 368 loss: 3.8330118032172322 time: 237.93293261528015 s\n",
      "Epoch: 369 loss: 3.763500540982932 time: 237.95722317695618 s\n",
      "Epoch: 370 loss: 3.75692012347281 time: 237.90991759300232 s\n",
      "Epoch: 371 loss: 3.6993210869841278 time: 237.90945315361023 s\n",
      "Checkpoint saved\n",
      "Epoch: 372 loss: 3.727281963452697 time: 237.93618369102478 s\n",
      "Epoch: 373 loss: 3.757761113345623 time: 237.9176561832428 s\n",
      "Epoch: 374 loss: 3.8357441034168005 time: 238.01154232025146 s\n",
      "Epoch: 375 loss: 3.834840157534927 time: 237.93429374694824 s\n",
      "Epoch: 376 loss: 3.72947380784899 time: 237.83316659927368 s\n",
      "Epoch: 377 loss: 3.6375322197563946 time: 237.92313504219055 s\n",
      "Checkpoint saved\n",
      "Epoch: 378 loss: 3.5991888246499 time: 237.86762762069702 s\n",
      "Checkpoint saved\n",
      "Epoch: 379 loss: 3.6230952790938318 time: 237.94856810569763 s\n",
      "Epoch: 380 loss: 3.6794544854201376 time: 237.94164299964905 s\n",
      "Epoch: 381 loss: 3.6885277181863785 time: 237.95157170295715 s\n",
      "Epoch: 382 loss: 3.633900430519134 time: 237.86253261566162 s\n",
      "Epoch: 383 loss: 3.617775089573115 time: 237.81764125823975 s\n",
      "Epoch: 384 loss: 3.609735137782991 time: 237.87543940544128 s\n",
      "Epoch: 385 loss: 3.670967652462423 time: 237.8198561668396 s\n",
      "Epoch: 386 loss: 3.6886855638585985 time: 237.95444250106812 s\n",
      "Epoch: 387 loss: 3.659288318362087 time: 237.83250880241394 s\n",
      "Epoch: 388 loss: 3.591445595957339 time: 237.96859097480774 s\n",
      "Checkpoint saved\n",
      "Epoch: 389 loss: 3.595829900354147 time: 237.9687533378601 s\n",
      "Epoch: 390 loss: 3.6467910236679018 time: 237.83777618408203 s\n",
      "Epoch: 391 loss: 3.629662267398089 time: 237.83957767486572 s\n",
      "Epoch: 392 loss: 3.583504168316722 time: 237.88178706169128 s\n",
      "Checkpoint saved\n",
      "Epoch: 393 loss: 3.5734677966684103 time: 237.8771369457245 s\n",
      "Checkpoint saved\n",
      "Epoch: 394 loss: 3.5882302657701075 time: 237.79867577552795 s\n",
      "Epoch: 395 loss: 3.5841604904271662 time: 237.79976749420166 s\n",
      "Epoch: 396 loss: 3.589795491658151 time: 237.78988456726074 s\n",
      "Epoch: 397 loss: 3.562337286770344 time: 237.8775510787964 s\n",
      "Checkpoint saved\n",
      "Epoch: 398 loss: 3.6164512340910733 time: 238.00763130187988 s\n",
      "Epoch: 399 loss: 3.5402410780079663 time: 237.79921102523804 s\n",
      "Checkpoint saved\n",
      "Epoch: 400 loss: 3.5212522032670677 time: 237.87227082252502 s\n",
      "Checkpoint saved\n",
      "Epoch: 401 loss: 3.5464372895658016 time: 237.9360659122467 s\n",
      "Epoch: 402 loss: 3.6915215449407697 time: 237.83798170089722 s\n",
      "Epoch: 403 loss: 3.5932677234523 time: 237.89142394065857 s\n",
      "Epoch: 404 loss: 3.48375724023208 time: 237.85577392578125 s\n",
      "Checkpoint saved\n",
      "Epoch: 405 loss: 3.5530191590078175 time: 237.82378244400024 s\n",
      "Epoch: 406 loss: 3.556589212734252 time: 237.77964448928833 s\n",
      "Epoch: 407 loss: 3.536113361362368 time: 237.74135327339172 s\n",
      "Epoch: 408 loss: 3.488658662419766 time: 237.8496491909027 s\n",
      "Epoch: 409 loss: 3.498743907548487 time: 237.73245072364807 s\n",
      "Epoch: 410 loss: 3.5322949793189764 time: 237.8904311656952 s\n",
      "Epoch: 411 loss: 3.4543487392365932 time: 238.0064525604248 s\n",
      "Checkpoint saved\n",
      "Epoch: 412 loss: 3.50030110636726 time: 237.87063932418823 s\n",
      "Epoch: 413 loss: 3.527342364192009 time: 237.93164539337158 s\n",
      "Epoch: 414 loss: 3.522594179958105 time: 237.81489777565002 s\n",
      "Epoch: 415 loss: 3.509389038197696 time: 237.8569781780243 s\n",
      "Epoch: 416 loss: 3.716316402424127 time: 237.84481620788574 s\n",
      "Epoch: 417 loss: 3.563599770423025 time: 237.86906933784485 s\n",
      "Epoch: 418 loss: 3.444884620141238 time: 237.87748408317566 s\n",
      "Checkpoint saved\n",
      "Epoch: 419 loss: 3.393743383232504 time: 237.9108693599701 s\n",
      "Checkpoint saved\n",
      "Epoch: 420 loss: 3.3382652257569134 time: 237.86606431007385 s\n",
      "Checkpoint saved\n",
      "Epoch: 421 loss: 3.3794066947884858 time: 237.8410108089447 s\n",
      "Epoch: 422 loss: 3.4091414595022798 time: 237.86704063415527 s\n",
      "Epoch: 423 loss: 3.4016321091912687 time: 237.85890126228333 s\n",
      "Epoch: 424 loss: 3.423968119546771 time: 237.75281357765198 s\n",
      "Epoch: 425 loss: 3.4451393145136535 time: 237.84963035583496 s\n",
      "Epoch: 426 loss: 3.4291625288315117 time: 237.80727529525757 s\n",
      "Epoch: 427 loss: 3.430176606401801 time: 237.9139244556427 s\n",
      "Epoch: 428 loss: 9.360915417782962 time: 237.8548514842987 s\n",
      "Epoch: 429 loss: 7.624321565032005 time: 237.831778049469 s\n",
      "Epoch: 430 loss: 5.055972224101424 time: 237.93557691574097 s\n",
      "Epoch: 431 loss: 4.230361977126449 time: 237.99023842811584 s\n",
      "Epoch: 432 loss: 3.7650031000375748 time: 237.86555790901184 s\n",
      "Epoch: 433 loss: 3.4449207787401974 time: 238.17536211013794 s\n",
      "Epoch: 434 loss: 3.332820245064795 time: 237.88985180854797 s\n",
      "Checkpoint saved\n",
      "Epoch: 435 loss: 3.2890868419781327 time: 237.99347853660583 s\n",
      "Checkpoint saved\n",
      "Epoch: 436 loss: 3.13812504010275 time: 237.99487257003784 s\n",
      "Checkpoint saved\n",
      "Epoch: 437 loss: 3.1247385474853218 time: 237.9654347896576 s\n",
      "Checkpoint saved\n",
      "Epoch: 438 loss: 3.154749878682196 time: 238.15656232833862 s\n",
      "Epoch: 439 loss: 3.1357024307362735 time: 237.9431357383728 s\n",
      "Epoch: 440 loss: 3.1431459435261786 time: 238.043315410614 s\n",
      "Epoch: 441 loss: 3.168470914941281 time: 238.0649755001068 s\n",
      "Epoch: 442 loss: 3.2417858564294875 time: 238.05018639564514 s\n",
      "Epoch: 443 loss: 3.348055145703256 time: 238.07743430137634 s\n",
      "Epoch: 444 loss: 3.3245469564571977 time: 238.17575120925903 s\n",
      "Epoch: 445 loss: 3.332563654985279 time: 238.07703948020935 s\n",
      "Epoch: 446 loss: 3.2819604971446097 time: 240.57306933403015 s\n",
      "Epoch: 447 loss: 3.3231901829130948 time: 238.32770133018494 s\n",
      "Epoch: 448 loss: 3.342229490634054 time: 238.21964716911316 s\n",
      "Epoch: 449 loss: 3.324708423577249 time: 238.02887845039368 s\n",
      "Epoch: 450 loss: 3.420617923606187 time: 238.08361864089966 s\n",
      "Epoch: 451 loss: 3.374798226170242 time: 238.02727890014648 s\n",
      "Epoch: 452 loss: 3.363423270639032 time: 238.07272672653198 s\n",
      "Epoch: 453 loss: 3.310669530183077 time: 238.0353560447693 s\n",
      "Epoch: 454 loss: 3.439541912637651 time: 238.02640318870544 s\n",
      "Epoch: 455 loss: 3.3456012564711273 time: 238.13358163833618 s\n",
      "Epoch: 456 loss: 3.323021066840738 time: 238.0606484413147 s\n",
      "Epoch: 457 loss: 3.3411175617948174 time: 237.98103737831116 s\n",
      "Epoch: 458 loss: 3.3715171455405653 time: 237.9275918006897 s\n",
      "Epoch: 459 loss: 3.379428034648299 time: 238.09595823287964 s\n",
      "Epoch: 460 loss: 3.324222030583769 time: 238.1113998889923 s\n",
      "Epoch: 461 loss: 3.339971940033138 time: 238.05653166770935 s\n",
      "Epoch: 462 loss: 3.29731379263103 time: 238.0307161808014 s\n",
      "Epoch: 463 loss: 3.4158536829054356 time: 238.014723777771 s\n",
      "Epoch: 464 loss: 3.3438485031947494 time: 238.09498977661133 s\n",
      "Epoch: 465 loss: 3.2999226949177682 time: 237.98353719711304 s\n",
      "Epoch: 466 loss: 3.288347657304257 time: 238.000066280365 s\n",
      "Epoch: 467 loss: 3.2843415862880647 time: 238.07115268707275 s\n",
      "Epoch: 468 loss: 3.3565645376220345 time: 238.118896484375 s\n",
      "Epoch: 469 loss: 3.333218869753182 time: 238.1574432849884 s\n",
      "Epoch: 470 loss: 3.2504960442893207 time: 238.04957151412964 s\n",
      "Epoch: 471 loss: 3.1975335483439267 time: 238.04226183891296 s\n",
      "Epoch: 472 loss: 3.3431650139391422 time: 238.08527612686157 s\n",
      "Epoch: 473 loss: 3.2729592379182577 time: 238.1092870235443 s\n",
      "Epoch: 474 loss: 3.2528895027935505 time: 238.0765082836151 s\n",
      "Epoch: 475 loss: 3.2706477721221745 time: 237.97887229919434 s\n",
      "Epoch: 476 loss: 3.336798809468746 time: 238.18801832199097 s\n",
      "Epoch: 477 loss: 3.3449015100486577 time: 237.9804220199585 s\n",
      "Epoch: 478 loss: 3.207120444159955 time: 238.07561588287354 s\n",
      "Epoch: 479 loss: 3.1966058118268847 time: 238.10966157913208 s\n",
      "Epoch: 480 loss: 3.18024229304865 time: 237.89628887176514 s\n",
      "Epoch: 481 loss: 3.1568274055607617 time: 237.91322016716003 s\n",
      "Epoch: 482 loss: 3.193927332293242 time: 237.9348030090332 s\n",
      "Epoch: 483 loss: 3.1895988271571696 time: 238.02722930908203 s\n",
      "Epoch: 484 loss: 3.1838878677226603 time: 238.19990372657776 s\n",
      "Epoch: 485 loss: 3.354222404304892 time: 238.01959419250488 s\n",
      "Epoch: 486 loss: 3.2968650204129517 time: 238.07249188423157 s\n",
      "Epoch: 487 loss: 3.1859687948599458 time: 238.01973056793213 s\n",
      "Epoch: 488 loss: 3.2338513624854386 time: 237.87062168121338 s\n",
      "Epoch: 489 loss: 3.1484067658893764 time: 237.91916632652283 s\n",
      "Epoch: 490 loss: 3.1882978905923665 time: 237.99595522880554 s\n",
      "Epoch: 491 loss: 3.1525356275960803 time: 237.99639534950256 s\n",
      "Epoch: 492 loss: 3.2024244838394225 time: 237.98699378967285 s\n",
      "Epoch: 493 loss: 3.1834040815010667 time: 237.9771933555603 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 494 loss: 3.2037081569433212 time: 238.06627750396729 s\n",
      "Epoch: 495 loss: 3.174900853075087 time: 238.00922298431396 s\n",
      "Epoch: 496 loss: 3.198228213004768 time: 238.017418384552 s\n",
      "Epoch: 497 loss: 3.197917287237942 time: 237.92114543914795 s\n",
      "Epoch: 498 loss: 3.1813455750234425 time: 237.9789218902588 s\n",
      "Epoch: 499 loss: 3.1363280713558197 time: 237.94813179969788 s\n",
      "Epoch: 500 loss: 3.1740955845452845 time: 237.96405005455017 s\n",
      "Finished Training\n",
      "[43.44235575199127, 26.578725691884756, 23.246523153036833, 19.86813148483634, 18.983006946742535, 17.822439590469003, 16.21480198018253, 15.275330897420645, 15.054051768034697, 14.218648981302977, 13.643306516110897, 13.2335836738348, 12.904990680515766, 12.788451502099633, 13.906510373577476, 13.411184709519148, 12.194152789190412, 11.928161235526204, 11.53395333327353, 11.228587793186307, 11.050389908254147, 10.796904236078262, 10.676769684068859, 11.225154586136341, 10.61463712155819, 10.422103626653552, 10.12984485924244, 10.01987105421722, 9.919281216338277, 9.926491309888661, 10.303645445033908, 10.009856978431344, 9.580813409760594, 10.250194570049644, 9.566440895199776, 9.185418095439672, 8.93639279063791, 8.972767231985927, 9.075321046635509, 9.050327070057392, 8.865316407755017, 8.812867322936654, 8.71034175530076, 8.727871754206717, 8.648622347041965, 8.831802627071738, 8.45689260493964, 8.347211811691523, 8.264489501714706, 8.256863093934953, 8.241939120925963, 9.216427901759744, 8.883908730000257, 7.984007993713021, 7.815881684422493, 7.7285719234496355, 7.691472572274506, 7.778903527185321, 7.835460786707699, 7.821797102689743, 7.726347237825394, 7.724363472312689, 7.766858397983015, 7.625623513944447, 7.460424146614969, 7.587259360589087, 7.4531710129231215, 7.4629790261387825, 7.476153532974422, 7.419022376649082, 7.243162431754172, 7.429777830839157, 7.41551074013114, 7.223402678966522, 7.198628956452012, 7.142257200554013, 7.129398279823363, 7.049991776235402, 7.106896111741662, 7.028609343804419, 6.988732525147498, 6.884199399501085, 6.839349782094359, 6.796832143329084, 6.8031261414289474, 6.828702569939196, 6.744442189112306, 6.746331445872784, 6.697208418510854, 6.66256685834378, 6.675255413167179, 6.580938398838043, 6.580907016992569, 6.6503822058439255, 6.790206839330494, 6.5371803771704435, 6.398900089785457, 6.341873464174569, 6.404909861274064, 6.467316967435181, 6.336922904476523, 6.2839479967951775, 6.579148298129439, 6.332511030137539, 6.239242254756391, 6.321410548873246, 6.214000143110752, 6.157045365311205, 6.577893728390336, 6.196667739190161, 6.088874721899629, 6.004361207596958, 6.021784477867186, 9.933063636533916, 14.416956325992942, 9.021850612014532, 7.390742231160402, 6.671435027383268, 6.249731785617769, 5.965912719257176, 5.825400850735605, 5.787101532332599, 5.734213276766241, 5.663996180519462, 5.675771647132933, 5.6344291446730494, 5.706817157566547, 5.795093893073499, 5.801473788917065, 5.916758148930967, 5.79971569404006, 6.302787019871175, 5.883102803491056, 5.717168688774109, 5.864156569354236, 5.8204139759764075, 5.905370294116437, 5.71896611712873, 5.69326036144048, 13.500255171209574, 12.447086714208126, 8.501145675778389, 7.424677713774145, 6.554753699339926, 6.156108145602047, 6.097380544990301, 5.795159287750721, 5.6054606242105365, 5.529441928490996, 5.58338469825685, 5.463867777958512, 5.360838007181883, 5.317832936532795, 5.393369322642684, 5.410265335813165, 5.474158146418631, 5.423547059297562, 5.504152677953243, 5.596398565918207, 5.611076805740595, 5.542944949120283, 5.5333831226453185, 5.640198443084955, 5.723412949591875, 5.636745364405215, 5.4880452221259475, 5.552523782476783, 5.501467132940888, 5.524153656326234, 5.426237167790532, 5.45877649076283, 5.393241840414703, 5.427766557782888, 5.417201014235616, 5.443211079575121, 5.379980232566595, 5.331669512204826, 5.480966788716614, 5.368954408913851, 5.312410110607743, 5.232785559259355, 5.246355445124209, 5.335435562767088, 5.207907820120454, 5.197244670242071, 5.208580375649035, 5.1936766700819135, 5.191138248890638, 5.200387045741081, 5.178549587726593, 5.136019458062947, 5.127057857811451, 5.149624979123473, 5.05967041105032, 5.0785236563533545, 5.077227904461324, 5.100759938359261, 5.095373994670808, 5.188128314912319, 5.068397262133658, 5.0418323846533895, 4.9644846664741635, 4.974885412491858, 4.92938378918916, 4.899456788785756, 5.002331337891519, 4.9592719580978155, 4.888789814896882, 4.857682866975665, 5.005078982561827, 5.050164264626801, 4.888041107915342, 4.853685106150806, 4.793487461283803, 4.8976012747734785, 4.7733732759952545, 4.737760240677744, 4.785794283263385, 4.861902320757508, 4.8094439171254635, 4.810253539122641, 4.7562357652932405, 4.712190763093531, 4.698953030630946, 6.296198350377381, 4.989405850879848, 4.568616348318756, 4.458643603138626, 4.430087707005441, 4.4611123451031744, 4.514394306577742, 4.557390362024307, 4.6151003846898675, 7.7360037956386805, 7.5586570631712675, 5.391358148306608, 4.783413153141737, 4.598501395434141, 4.41332864202559, 4.445261163637042, 4.319328165613115, 4.297984664328396, 4.447765822522342, 4.400516036432236, 4.459273950196803, 4.451233308296651, 4.541255367454141, 4.580882400274277, 4.55484358407557, 4.566498324740678, 4.630310460925102, 4.621210774406791, 4.603346002288163, 4.546672070398927, 4.479232859797776, 4.443494176492095, 4.502584814094007, 4.511824043467641, 4.50666787661612, 4.481130784377456, 4.441737083252519, 4.447602664120495, 4.473528936505318, 4.519556704908609, 4.418503220193088, 4.4210687428712845, 4.423022690229118, 4.367261667735875, 4.353063603397459, 4.4258070779033005, 4.372231228277087, 4.382296248339117, 4.372199513949454, 4.331389430910349, 4.7749184696003795, 7.814909032545984, 5.204270397312939, 4.429662648588419, 4.223525008186698, 4.097603005822748, 4.029993735719472, 4.039309639018029, 4.083584425970912, 4.120041054673493, 4.14740731427446, 4.339109733700752, 4.218724534381181, 4.230850362684578, 4.301375960931182, 4.308142859488726, 4.322224681265652, 4.2274228017777205, 4.2123179994523525, 4.201879889704287, 4.2475810456089675, 4.184194983448833, 4.151008595246822, 4.193691075779498, 4.206002940423787, 4.172820877749473, 4.225586014334112, 4.2151899533346295, 4.2471964112482965, 4.128755924291909, 4.092297750990838, 4.101247769780457, 4.124775668606162, 4.065557694993913, 4.123529034666717, 4.138474840205163, 4.065314275678247, 4.081845776177943, 4.077884249854833, 4.055064530111849, 4.0710079236887395, 4.752585367765278, 4.123210360761732, 3.9591430397704244, 3.9158590119332075, 4.016450149938464, 3.974427575711161, 4.090918689966202, 3.995254924055189, 3.932042765431106, 3.9773074840195477, 4.3370077782310545, 4.239965740125626, 4.106882211286575, 3.8793691163882613, 3.8593199173919857, 3.8590629622340202, 3.9252982125617564, 4.0219859653152525, 3.904655340593308, 3.968285445123911, 3.9003797117620707, 4.152231497690082, 3.953596828505397, 3.856064728461206, 3.841331632807851, 3.838098296429962, 3.8275233483873308, 3.9176908265799284, 3.927123815752566, 3.9261650913394988, 4.082094385754317, 4.2389938603155315, 3.863807258196175, 3.7431182274594903, 3.7363482895307243, 3.796165795996785, 3.8206403967924416, 3.8161935759708285, 3.7813343456946313, 3.7756877932697535, 3.811761349439621, 3.7825160548090935, 4.0913951667025685, 3.884636731352657, 3.70932662114501, 3.756547389551997, 3.790770314168185, 3.7846331386826932, 3.7220829273574054, 3.73157126782462, 3.735072623472661, 3.76908414112404, 3.8330118032172322, 3.763500540982932, 3.75692012347281, 3.6993210869841278, 3.727281963452697, 3.757761113345623, 3.8357441034168005, 3.834840157534927, 3.72947380784899, 3.6375322197563946, 3.5991888246499, 3.6230952790938318, 3.6794544854201376, 3.6885277181863785, 3.633900430519134, 3.617775089573115, 3.609735137782991, 3.670967652462423, 3.6886855638585985, 3.659288318362087, 3.591445595957339, 3.595829900354147, 3.6467910236679018, 3.629662267398089, 3.583504168316722, 3.5734677966684103, 3.5882302657701075, 3.5841604904271662, 3.589795491658151, 3.562337286770344, 3.6164512340910733, 3.5402410780079663, 3.5212522032670677, 3.5464372895658016, 3.6915215449407697, 3.5932677234523, 3.48375724023208, 3.5530191590078175, 3.556589212734252, 3.536113361362368, 3.488658662419766, 3.498743907548487, 3.5322949793189764, 3.4543487392365932, 3.50030110636726, 3.527342364192009, 3.522594179958105, 3.509389038197696, 3.716316402424127, 3.563599770423025, 3.444884620141238, 3.393743383232504, 3.3382652257569134, 3.3794066947884858, 3.4091414595022798, 3.4016321091912687, 3.423968119546771, 3.4451393145136535, 3.4291625288315117, 3.430176606401801, 9.360915417782962, 7.624321565032005, 5.055972224101424, 4.230361977126449, 3.7650031000375748, 3.4449207787401974, 3.332820245064795, 3.2890868419781327, 3.13812504010275, 3.1247385474853218, 3.154749878682196, 3.1357024307362735, 3.1431459435261786, 3.168470914941281, 3.2417858564294875, 3.348055145703256, 3.3245469564571977, 3.332563654985279, 3.2819604971446097, 3.3231901829130948, 3.342229490634054, 3.324708423577249, 3.420617923606187, 3.374798226170242, 3.363423270639032, 3.310669530183077, 3.439541912637651, 3.3456012564711273, 3.323021066840738, 3.3411175617948174, 3.3715171455405653, 3.379428034648299, 3.324222030583769, 3.339971940033138, 3.29731379263103, 3.4158536829054356, 3.3438485031947494, 3.2999226949177682, 3.288347657304257, 3.2843415862880647, 3.3565645376220345, 3.333218869753182, 3.2504960442893207, 3.1975335483439267, 3.3431650139391422, 3.2729592379182577, 3.2528895027935505, 3.2706477721221745, 3.336798809468746, 3.3449015100486577, 3.207120444159955, 3.1966058118268847, 3.18024229304865, 3.1568274055607617, 3.193927332293242, 3.1895988271571696, 3.1838878677226603, 3.354222404304892, 3.2968650204129517, 3.1859687948599458, 3.2338513624854386, 3.1484067658893764, 3.1882978905923665, 3.1525356275960803, 3.2024244838394225, 3.1834040815010667, 3.2037081569433212, 3.174900853075087, 3.198228213004768, 3.197917287237942, 3.1813455750234425, 3.1363280713558197, 3.1740955845452845]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "\n",
    "losses = []\n",
    "best_loss = 1e19\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    s_time = time.time()\n",
    "    for data in train_loader:\n",
    "        # get the inputs\n",
    "        img = data[0]\n",
    "        trg = data[1]\n",
    "        img = img.type(torch.cuda.FloatTensor)\n",
    "        trg = trg.type(torch.cuda.FloatTensor)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred = model(img)\n",
    "        loss = loss_function(pred, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch:\", epoch+1, \"loss:\", running_loss, \"time:\", time.time()-s_time, \"s\")\n",
    "    if running_loss < best_loss:\n",
    "        torch.save(model.state_dict(), model_path[:-4]+'_cpt'+model_path[-4:])\n",
    "        best_loss = running_loss\n",
    "        print(\"Checkpoint saved\")\n",
    "    losses.append(running_loss)\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 650\n",
    "img_test = cv2.imread('./data/CameraRGB/'+str(idx)+'.png')\n",
    "show_img(img_test)\n",
    "trg_test = train_dataset._fix_trg(cv2.imread('./data/CameraSeg/'+str(idx)+'.png'))\n",
    "show_img(trg_test)\n",
    "img_test = np.moveaxis(img_test, -1, 0)\n",
    "img_test = img_test[np.newaxis,:,:,:]\n",
    "img_test = torch.from_numpy(img_test).type(torch.cuda.FloatTensor)\n",
    "pred = model(img_test)\n",
    "pred = pred.cpu().data[0,:,:,:].numpy()\n",
    "\n",
    "pred_img = np.moveaxis(pred, 0, -1)\n",
    "\n",
    "show_img(pred_img)\n",
    "show_img(np.abs(trg_test-pred_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "img_test = cv2.imread('./data/CameraRGB/'+str(idx)+'.png')\n",
    "img_test = np.moveaxis(img_test, -1, 0)\n",
    "img_test = img_test[np.newaxis,:,:,:]\n",
    "img_test = torch.from_numpy(img_test).type(torch.cuda.FloatTensor)\n",
    "pred = model(img_test)\n",
    "pred = pred.cpu().data[0,:,:,:].numpy()\n",
    "pred_img = np.moveaxis(pred, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
